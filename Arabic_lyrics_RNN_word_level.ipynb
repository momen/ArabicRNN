{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import problem_unittests as tests\n",
    "\n",
    "data_dir = 'aggregate_lyrics.txt'\n",
    "text = helper.load_data(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 109977\n",
      "The sentences 0 to 10:\n",
      "اروح لاحبابي و الاقي الفرح ساكن عينهم \n",
      " ابتسم لافراحهم و انا من الهم احترق \n",
      " و اسال جروحي من تري حس بعذابي منهم \n",
      " و بالحقيقه انصدم محدن معه همي فرق \n",
      "دورت في كل الوجيه حسيت غربه بينهم \n",
      " مع الاسف محدن ابد حس بعذاباتي و رق \n",
      " جيت اتعثر بالتعب ابي اشوف يدينهم \n",
      " ماكنت ابي الا احد يحس بي لو مانطق \n",
      " و حز فيني اني رجعت لكن رجعت بدونهم \n",
      " يحز في نفسي بانه ماسوي جرحي صدق \n"
     ]
    }
   ],
   "source": [
    "view_sentence_range = (0, 10)\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "print('The sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of lyrics split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    \n",
    "    word_counts = set(text)\n",
    "    int_to_vocab = {ii: word for ii, word in enumerate(word_counts)}\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}    \n",
    "    \n",
    "    return (vocab_to_int, int_to_vocab)\n",
    "tests.test_create_lookup_tables(create_lookup_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenize dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    tokens = {'.':'||Period||',\n",
    "              ',':'||Comma||',\n",
    "              '\"':'||Quotation_Mark||',\n",
    "              ';':'||Semicolon||',\n",
    "              '!':'||Exclamation_Mark||',\n",
    "              '؟':'||Question_Mark||',\n",
    "              '(':'||Left_Parantheses||',\n",
    "              ')':'||Right_Parantheses||',\n",
    "              '--':'||Dash||',\n",
    "              '\\n':'||Return||'\n",
    "             }\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint. If you ever decide to come back to this notebook or have to restart the notebook, you can start from here. The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.10.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, and learning rate.\n",
    "    :return: Tuple (input, targets, learning rate)\n",
    "    \"\"\"\n",
    "    inputs         = tf.placeholder(tf.int32,[None, None], name='input')\n",
    "    targets        = tf.placeholder(tf.int32, [None, None], name='target')\n",
    "    learning_rate  = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    return (inputs, targets, learning_rate)\n",
    "tests.test_get_inputs(get_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_init_cell(batch_size, rnn_size):\n",
    "    \"\"\"\n",
    "    Create an RNN Cell and initialize it.\n",
    "    :param batch_size: Size of batches\n",
    "    :param rnn_size: Size of RNNs\n",
    "    :return: Tuple (cell, initialize state)\n",
    "    \"\"\"\n",
    "   \n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    lstm_layers = 1\n",
    "    Cell = tf.contrib.rnn.MultiRNNCell([lstm] * lstm_layers)\n",
    "    InitialState = Cell.zero_state(batch_size, tf.float32)\n",
    "    InitialState = tf.identity(InitialState, name='initial_state')\n",
    "    \n",
    "    return (Cell, InitialState)\n",
    "tests.test_get_init_cell(get_init_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Create embedding for <input_data>.\n",
    "    :param input_data: TF placeholder for text input.\n",
    "    :param vocab_size: Number of words in vocabulary.\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Embedded input.\n",
    "    \"\"\"\n",
    "    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1), dtype=tf.float32)\n",
    "    embed = tf.nn.embedding_lookup(embedding, input_data)\n",
    "    \n",
    "    return embed\n",
    "tests.test_get_embed(get_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    \"\"\"\n",
    "    Create a RNN using a RNN Cell\n",
    "    :param cell: RNN Cell\n",
    "    :param inputs: Input text data\n",
    "    :return: Tuple (Outputs, Final State)\n",
    "    \"\"\"\n",
    "\n",
    "    Outputs, FinalState = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32) \n",
    "    FinalState =  tf.identity(FinalState, name='final_state')\n",
    "    return (Outputs, FinalState)\n",
    "tests.test_build_rnn(build_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size,embed_dim):\n",
    "    \"\"\"\n",
    "    Build part of the neural network\n",
    "    :param cell: RNN cell\n",
    "    :param rnn_size: Size of rnns\n",
    "    :param input_data: Input data\n",
    "    :param vocab_size: Vocabulary size\n",
    "    :return: Tuple (Logits, FinalState)\n",
    "    \"\"\"\n",
    "    \n",
    "    embedded = get_embed(input_data=input_data, vocab_size=vocab_size,embed_dim=200)\n",
    "    outputs, FinalState = build_rnn(cell=cell, inputs=embedded)\n",
    "    batch_size, embed_size = input_data.get_shape()\n",
    "    logits = tf.contrib.layers.fully_connected(outputs, vocab_size, activation_fn=None)\n",
    "\n",
    "    return (logits, FinalState)\n",
    "tests.test_build_nn(build_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target\n",
    "    :param int_text: Text with the words replaced by their ids\n",
    "    :param batch_size: The size of batch\n",
    "    :param seq_length: The length of sequence\n",
    "    :return: Batches as a Numpy array\n",
    "    \"\"\"\n",
    "    n_batches = len(int_text)//(batch_size*seq_length)\n",
    "\n",
    "    valid_text = int_text[:n_batches*batch_size*seq_length+1]\n",
    "    \n",
    "    result = np.ndarray((n_batches,2,batch_size,seq_length), dtype=int)\n",
    "    step = n_batches*seq_length    \n",
    "    \n",
    "    #print(valid_text)\n",
    "    \n",
    "    for batch in range(n_batches):\n",
    "        batch_walk = batch*seq_length\n",
    "        x = []\n",
    "        y = []\n",
    "        for binn in range(batch_size):\n",
    "            idx = batch_walk + binn * step    # start from this index\n",
    "            result[batch][0][binn] = valid_text[idx   : idx    +seq_length]\n",
    "            result[batch][1][binn] = valid_text[idx+1 : idx+1  +seq_length]   \n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "rnn_size = 256\n",
    "seq_length = 50\n",
    "learning_rate = 0.01\n",
    "show_every_n_batches = 10\n",
    "embed_dim = 200\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, lr = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size)\n",
    "    logits, final_state = build_nn(cell,rnn_size, input_text, vocab_size,embed_dim)\n",
    "\n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Train the neural network on the preprocessed data.  If you have a hard time getting a good loss, check the [forms](https://discussions.udacity.com/) to see if anyone is having the same problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/305   train_loss = 11.607\n",
      "Epoch   0 Batch   10/305   train_loss = 8.802\n",
      "Epoch   0 Batch   20/305   train_loss = 8.664\n",
      "Epoch   0 Batch   30/305   train_loss = 8.546\n",
      "Epoch   0 Batch   40/305   train_loss = 8.314\n",
      "Epoch   0 Batch   50/305   train_loss = 8.416\n",
      "Epoch   0 Batch   60/305   train_loss = 8.222\n",
      "Epoch   0 Batch   70/305   train_loss = 7.978\n",
      "Epoch   0 Batch   80/305   train_loss = 7.710\n",
      "Epoch   0 Batch   90/305   train_loss = 7.834\n",
      "Epoch   0 Batch  100/305   train_loss = 7.779\n",
      "Epoch   0 Batch  110/305   train_loss = 7.712\n",
      "Epoch   0 Batch  120/305   train_loss = 7.548\n",
      "Epoch   0 Batch  130/305   train_loss = 7.616\n",
      "Epoch   0 Batch  140/305   train_loss = 7.402\n",
      "Epoch   0 Batch  150/305   train_loss = 7.413\n",
      "Epoch   0 Batch  160/305   train_loss = 7.186\n",
      "Epoch   0 Batch  170/305   train_loss = 7.276\n",
      "Epoch   0 Batch  180/305   train_loss = 7.095\n",
      "Epoch   0 Batch  190/305   train_loss = 6.969\n",
      "Epoch   0 Batch  200/305   train_loss = 7.202\n",
      "Epoch   0 Batch  210/305   train_loss = 7.110\n",
      "Epoch   0 Batch  220/305   train_loss = 7.054\n",
      "Epoch   0 Batch  230/305   train_loss = 7.049\n",
      "Epoch   0 Batch  240/305   train_loss = 7.084\n",
      "Epoch   0 Batch  250/305   train_loss = 6.889\n",
      "Epoch   0 Batch  260/305   train_loss = 6.933\n",
      "Epoch   0 Batch  270/305   train_loss = 6.901\n",
      "Epoch   0 Batch  280/305   train_loss = 6.946\n",
      "Epoch   0 Batch  290/305   train_loss = 6.919\n",
      "Epoch   0 Batch  300/305   train_loss = 6.877\n",
      "Epoch   1 Batch    5/305   train_loss = 6.843\n",
      "Epoch   1 Batch   15/305   train_loss = 6.914\n",
      "Epoch   1 Batch   25/305   train_loss = 6.893\n",
      "Epoch   1 Batch   35/305   train_loss = 6.948\n",
      "Epoch   1 Batch   45/305   train_loss = 6.814\n",
      "Epoch   1 Batch   55/305   train_loss = 6.868\n",
      "Epoch   1 Batch   65/305   train_loss = 6.858\n",
      "Epoch   1 Batch   75/305   train_loss = 6.788\n",
      "Epoch   1 Batch   85/305   train_loss = 6.687\n",
      "Epoch   1 Batch   95/305   train_loss = 6.600\n",
      "Epoch   1 Batch  105/305   train_loss = 6.694\n",
      "Epoch   1 Batch  115/305   train_loss = 6.625\n",
      "Epoch   1 Batch  125/305   train_loss = 6.701\n",
      "Epoch   1 Batch  135/305   train_loss = 6.618\n",
      "Epoch   1 Batch  145/305   train_loss = 6.622\n",
      "Epoch   1 Batch  155/305   train_loss = 6.509\n",
      "Epoch   1 Batch  165/305   train_loss = 6.428\n",
      "Epoch   1 Batch  175/305   train_loss = 6.519\n",
      "Epoch   1 Batch  185/305   train_loss = 6.359\n",
      "Epoch   1 Batch  195/305   train_loss = 6.329\n",
      "Epoch   1 Batch  205/305   train_loss = 6.373\n",
      "Epoch   1 Batch  215/305   train_loss = 6.421\n",
      "Epoch   1 Batch  225/305   train_loss = 6.363\n",
      "Epoch   1 Batch  235/305   train_loss = 6.454\n",
      "Epoch   1 Batch  245/305   train_loss = 6.270\n",
      "Epoch   1 Batch  255/305   train_loss = 6.301\n",
      "Epoch   1 Batch  265/305   train_loss = 6.231\n",
      "Epoch   1 Batch  275/305   train_loss = 6.149\n",
      "Epoch   1 Batch  285/305   train_loss = 6.183\n",
      "Epoch   1 Batch  295/305   train_loss = 6.172\n",
      "Epoch   2 Batch    0/305   train_loss = 6.148\n",
      "Epoch   2 Batch   10/305   train_loss = 6.283\n",
      "Epoch   2 Batch   20/305   train_loss = 6.262\n",
      "Epoch   2 Batch   30/305   train_loss = 6.274\n",
      "Epoch   2 Batch   40/305   train_loss = 6.178\n",
      "Epoch   2 Batch   50/305   train_loss = 6.262\n",
      "Epoch   2 Batch   60/305   train_loss = 6.242\n",
      "Epoch   2 Batch   70/305   train_loss = 6.315\n",
      "Epoch   2 Batch   80/305   train_loss = 6.067\n",
      "Epoch   2 Batch   90/305   train_loss = 6.246\n",
      "Epoch   2 Batch  100/305   train_loss = 6.182\n",
      "Epoch   2 Batch  110/305   train_loss = 6.208\n",
      "Epoch   2 Batch  120/305   train_loss = 6.109\n",
      "Epoch   2 Batch  130/305   train_loss = 6.153\n",
      "Epoch   2 Batch  140/305   train_loss = 6.161\n",
      "Epoch   2 Batch  150/305   train_loss = 6.198\n",
      "Epoch   2 Batch  160/305   train_loss = 6.080\n",
      "Epoch   2 Batch  170/305   train_loss = 6.165\n",
      "Epoch   2 Batch  180/305   train_loss = 6.029\n",
      "Epoch   2 Batch  190/305   train_loss = 5.882\n",
      "Epoch   2 Batch  200/305   train_loss = 5.987\n",
      "Epoch   2 Batch  210/305   train_loss = 5.922\n",
      "Epoch   2 Batch  220/305   train_loss = 5.823\n",
      "Epoch   2 Batch  230/305   train_loss = 5.859\n",
      "Epoch   2 Batch  240/305   train_loss = 5.898\n",
      "Epoch   2 Batch  250/305   train_loss = 5.773\n",
      "Epoch   2 Batch  260/305   train_loss = 5.841\n",
      "Epoch   2 Batch  270/305   train_loss = 5.631\n",
      "Epoch   2 Batch  280/305   train_loss = 5.672\n",
      "Epoch   2 Batch  290/305   train_loss = 5.634\n",
      "Epoch   2 Batch  300/305   train_loss = 5.496\n",
      "Epoch   3 Batch    5/305   train_loss = 5.745\n",
      "Epoch   3 Batch   15/305   train_loss = 5.803\n",
      "Epoch   3 Batch   25/305   train_loss = 5.776\n",
      "Epoch   3 Batch   35/305   train_loss = 5.800\n",
      "Epoch   3 Batch   45/305   train_loss = 5.660\n",
      "Epoch   3 Batch   55/305   train_loss = 5.719\n",
      "Epoch   3 Batch   65/305   train_loss = 5.836\n",
      "Epoch   3 Batch   75/305   train_loss = 5.748\n",
      "Epoch   3 Batch   85/305   train_loss = 5.664\n",
      "Epoch   3 Batch   95/305   train_loss = 5.666\n",
      "Epoch   3 Batch  105/305   train_loss = 5.730\n",
      "Epoch   3 Batch  115/305   train_loss = 5.633\n",
      "Epoch   3 Batch  125/305   train_loss = 5.682\n",
      "Epoch   3 Batch  135/305   train_loss = 5.647\n",
      "Epoch   3 Batch  145/305   train_loss = 5.715\n",
      "Epoch   3 Batch  155/305   train_loss = 5.659\n",
      "Epoch   3 Batch  165/305   train_loss = 5.663\n",
      "Epoch   3 Batch  175/305   train_loss = 5.777\n",
      "Epoch   3 Batch  185/305   train_loss = 5.632\n",
      "Epoch   3 Batch  195/305   train_loss = 5.518\n",
      "Epoch   3 Batch  205/305   train_loss = 5.468\n",
      "Epoch   3 Batch  215/305   train_loss = 5.549\n",
      "Epoch   3 Batch  225/305   train_loss = 5.417\n",
      "Epoch   3 Batch  235/305   train_loss = 5.430\n",
      "Epoch   3 Batch  245/305   train_loss = 5.397\n",
      "Epoch   3 Batch  255/305   train_loss = 5.444\n",
      "Epoch   3 Batch  265/305   train_loss = 5.286\n",
      "Epoch   3 Batch  275/305   train_loss = 5.154\n",
      "Epoch   3 Batch  285/305   train_loss = 5.198\n",
      "Epoch   3 Batch  295/305   train_loss = 5.205\n",
      "Epoch   4 Batch    0/305   train_loss = 5.255\n",
      "Epoch   4 Batch   10/305   train_loss = 5.337\n",
      "Epoch   4 Batch   20/305   train_loss = 5.351\n",
      "Epoch   4 Batch   30/305   train_loss = 5.336\n",
      "Epoch   4 Batch   40/305   train_loss = 5.279\n",
      "Epoch   4 Batch   50/305   train_loss = 5.300\n",
      "Epoch   4 Batch   60/305   train_loss = 5.328\n",
      "Epoch   4 Batch   70/305   train_loss = 5.390\n",
      "Epoch   4 Batch   80/305   train_loss = 5.335\n",
      "Epoch   4 Batch   90/305   train_loss = 5.387\n",
      "Epoch   4 Batch  100/305   train_loss = 5.373\n",
      "Epoch   4 Batch  110/305   train_loss = 5.294\n",
      "Epoch   4 Batch  120/305   train_loss = 5.269\n",
      "Epoch   4 Batch  130/305   train_loss = 5.252\n",
      "Epoch   4 Batch  140/305   train_loss = 5.313\n",
      "Epoch   4 Batch  150/305   train_loss = 5.360\n",
      "Epoch   4 Batch  160/305   train_loss = 5.305\n",
      "Epoch   4 Batch  170/305   train_loss = 5.453\n",
      "Epoch   4 Batch  180/305   train_loss = 5.309\n",
      "Epoch   4 Batch  190/305   train_loss = 5.108\n",
      "Epoch   4 Batch  200/305   train_loss = 5.138\n",
      "Epoch   4 Batch  210/305   train_loss = 5.085\n",
      "Epoch   4 Batch  220/305   train_loss = 5.045\n",
      "Epoch   4 Batch  230/305   train_loss = 5.042\n",
      "Epoch   4 Batch  240/305   train_loss = 5.092\n",
      "Epoch   4 Batch  250/305   train_loss = 5.044\n",
      "Epoch   4 Batch  260/305   train_loss = 5.122\n",
      "Epoch   4 Batch  270/305   train_loss = 4.924\n",
      "Epoch   4 Batch  280/305   train_loss = 4.929\n",
      "Epoch   4 Batch  290/305   train_loss = 4.866\n",
      "Epoch   4 Batch  300/305   train_loss = 4.854\n",
      "Epoch   5 Batch    5/305   train_loss = 4.932\n",
      "Epoch   5 Batch   15/305   train_loss = 5.017\n",
      "Epoch   5 Batch   25/305   train_loss = 5.069\n",
      "Epoch   5 Batch   35/305   train_loss = 4.996\n",
      "Epoch   5 Batch   45/305   train_loss = 4.885\n",
      "Epoch   5 Batch   55/305   train_loss = 4.908\n",
      "Epoch   5 Batch   65/305   train_loss = 5.094\n",
      "Epoch   5 Batch   75/305   train_loss = 5.031\n",
      "Epoch   5 Batch   85/305   train_loss = 5.058\n",
      "Epoch   5 Batch   95/305   train_loss = 4.953\n",
      "Epoch   5 Batch  105/305   train_loss = 5.033\n",
      "Epoch   5 Batch  115/305   train_loss = 4.955\n",
      "Epoch   5 Batch  125/305   train_loss = 5.016\n",
      "Epoch   5 Batch  135/305   train_loss = 4.991\n",
      "Epoch   5 Batch  145/305   train_loss = 4.995\n",
      "Epoch   5 Batch  155/305   train_loss = 5.029\n",
      "Epoch   5 Batch  165/305   train_loss = 5.079\n",
      "Epoch   5 Batch  175/305   train_loss = 5.123\n",
      "Epoch   5 Batch  185/305   train_loss = 4.995\n",
      "Epoch   5 Batch  195/305   train_loss = 4.871\n",
      "Epoch   5 Batch  205/305   train_loss = 4.858\n",
      "Epoch   5 Batch  215/305   train_loss = 4.856\n",
      "Epoch   5 Batch  225/305   train_loss = 4.810\n",
      "Epoch   5 Batch  235/305   train_loss = 4.825\n",
      "Epoch   5 Batch  245/305   train_loss = 4.845\n",
      "Epoch   5 Batch  255/305   train_loss = 4.916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5 Batch  265/305   train_loss = 4.740\n",
      "Epoch   5 Batch  275/305   train_loss = 4.622\n",
      "Epoch   5 Batch  285/305   train_loss = 4.646\n",
      "Epoch   5 Batch  295/305   train_loss = 4.701\n",
      "Epoch   6 Batch    0/305   train_loss = 4.748\n",
      "Epoch   6 Batch   10/305   train_loss = 4.738\n",
      "Epoch   6 Batch   20/305   train_loss = 4.783\n",
      "Epoch   6 Batch   30/305   train_loss = 4.783\n",
      "Epoch   6 Batch   40/305   train_loss = 4.764\n",
      "Epoch   6 Batch   50/305   train_loss = 4.682\n",
      "Epoch   6 Batch   60/305   train_loss = 4.755\n",
      "Epoch   6 Batch   70/305   train_loss = 4.827\n",
      "Epoch   6 Batch   80/305   train_loss = 4.863\n",
      "Epoch   6 Batch   90/305   train_loss = 4.833\n",
      "Epoch   6 Batch  100/305   train_loss = 4.852\n",
      "Epoch   6 Batch  110/305   train_loss = 4.771\n",
      "Epoch   6 Batch  120/305   train_loss = 4.760\n",
      "Epoch   6 Batch  130/305   train_loss = 4.712\n",
      "Epoch   6 Batch  140/305   train_loss = 4.828\n",
      "Epoch   6 Batch  150/305   train_loss = 4.770\n",
      "Epoch   6 Batch  160/305   train_loss = 4.718\n",
      "Epoch   6 Batch  170/305   train_loss = 4.940\n",
      "Epoch   6 Batch  180/305   train_loss = 4.854\n",
      "Epoch   6 Batch  190/305   train_loss = 4.633\n",
      "Epoch   6 Batch  200/305   train_loss = 4.596\n",
      "Epoch   6 Batch  210/305   train_loss = 4.572\n",
      "Epoch   6 Batch  220/305   train_loss = 4.550\n",
      "Epoch   6 Batch  230/305   train_loss = 4.594\n",
      "Epoch   6 Batch  240/305   train_loss = 4.621\n",
      "Epoch   6 Batch  250/305   train_loss = 4.627\n",
      "Epoch   6 Batch  260/305   train_loss = 4.663\n",
      "Epoch   6 Batch  270/305   train_loss = 4.536\n",
      "Epoch   6 Batch  280/305   train_loss = 4.529\n",
      "Epoch   6 Batch  290/305   train_loss = 4.421\n",
      "Epoch   6 Batch  300/305   train_loss = 4.443\n",
      "Epoch   7 Batch    5/305   train_loss = 4.564\n",
      "Epoch   7 Batch   15/305   train_loss = 4.554\n",
      "Epoch   7 Batch   25/305   train_loss = 4.612\n",
      "Epoch   7 Batch   35/305   train_loss = 4.587\n",
      "Epoch   7 Batch   45/305   train_loss = 4.483\n",
      "Epoch   7 Batch   55/305   train_loss = 4.462\n",
      "Epoch   7 Batch   65/305   train_loss = 4.598\n",
      "Epoch   7 Batch   75/305   train_loss = 4.582\n",
      "Epoch   7 Batch   85/305   train_loss = 4.616\n",
      "Epoch   7 Batch   95/305   train_loss = 4.470\n",
      "Epoch   7 Batch  105/305   train_loss = 4.608\n",
      "Epoch   7 Batch  115/305   train_loss = 4.559\n",
      "Epoch   7 Batch  125/305   train_loss = 4.551\n",
      "Epoch   7 Batch  135/305   train_loss = 4.549\n",
      "Epoch   7 Batch  145/305   train_loss = 4.568\n",
      "Epoch   7 Batch  155/305   train_loss = 4.624\n",
      "Epoch   7 Batch  165/305   train_loss = 4.658\n",
      "Epoch   7 Batch  175/305   train_loss = 4.634\n",
      "Epoch   7 Batch  185/305   train_loss = 4.566\n",
      "Epoch   7 Batch  195/305   train_loss = 4.452\n",
      "Epoch   7 Batch  205/305   train_loss = 4.438\n",
      "Epoch   7 Batch  215/305   train_loss = 4.423\n",
      "Epoch   7 Batch  225/305   train_loss = 4.379\n",
      "Epoch   7 Batch  235/305   train_loss = 4.432\n",
      "Epoch   7 Batch  245/305   train_loss = 4.465\n",
      "Epoch   7 Batch  255/305   train_loss = 4.577\n",
      "Epoch   7 Batch  265/305   train_loss = 4.334\n",
      "Epoch   7 Batch  275/305   train_loss = 4.288\n",
      "Epoch   7 Batch  285/305   train_loss = 4.282\n",
      "Epoch   7 Batch  295/305   train_loss = 4.316\n",
      "Epoch   8 Batch    0/305   train_loss = 4.380\n",
      "Epoch   8 Batch   10/305   train_loss = 4.379\n",
      "Epoch   8 Batch   20/305   train_loss = 4.384\n",
      "Epoch   8 Batch   30/305   train_loss = 4.333\n",
      "Epoch   8 Batch   40/305   train_loss = 4.439\n",
      "Epoch   8 Batch   50/305   train_loss = 4.325\n",
      "Epoch   8 Batch   60/305   train_loss = 4.367\n",
      "Epoch   8 Batch   70/305   train_loss = 4.446\n",
      "Epoch   8 Batch   80/305   train_loss = 4.498\n",
      "Epoch   8 Batch   90/305   train_loss = 4.442\n",
      "Epoch   8 Batch  100/305   train_loss = 4.424\n",
      "Epoch   8 Batch  110/305   train_loss = 4.388\n",
      "Epoch   8 Batch  120/305   train_loss = 4.412\n",
      "Epoch   8 Batch  130/305   train_loss = 4.349\n",
      "Epoch   8 Batch  140/305   train_loss = 4.471\n",
      "Epoch   8 Batch  150/305   train_loss = 4.422\n",
      "Epoch   8 Batch  160/305   train_loss = 4.342\n",
      "Epoch   8 Batch  170/305   train_loss = 4.531\n",
      "Epoch   8 Batch  180/305   train_loss = 4.438\n",
      "Epoch   8 Batch  190/305   train_loss = 4.305\n",
      "Epoch   8 Batch  200/305   train_loss = 4.263\n",
      "Epoch   8 Batch  210/305   train_loss = 4.239\n",
      "Epoch   8 Batch  220/305   train_loss = 4.183\n",
      "Epoch   8 Batch  230/305   train_loss = 4.242\n",
      "Epoch   8 Batch  240/305   train_loss = 4.269\n",
      "Epoch   8 Batch  250/305   train_loss = 4.305\n",
      "Epoch   8 Batch  260/305   train_loss = 4.381\n",
      "Epoch   8 Batch  270/305   train_loss = 4.261\n",
      "Epoch   8 Batch  280/305   train_loss = 4.205\n",
      "Epoch   8 Batch  290/305   train_loss = 4.139\n",
      "Epoch   8 Batch  300/305   train_loss = 4.123\n",
      "Epoch   9 Batch    5/305   train_loss = 4.211\n",
      "Epoch   9 Batch   15/305   train_loss = 4.240\n",
      "Epoch   9 Batch   25/305   train_loss = 4.308\n",
      "Epoch   9 Batch   35/305   train_loss = 4.209\n",
      "Epoch   9 Batch   45/305   train_loss = 4.170\n",
      "Epoch   9 Batch   55/305   train_loss = 4.177\n",
      "Epoch   9 Batch   65/305   train_loss = 4.259\n",
      "Epoch   9 Batch   75/305   train_loss = 4.242\n",
      "Epoch   9 Batch   85/305   train_loss = 4.283\n",
      "Epoch   9 Batch   95/305   train_loss = 4.194\n",
      "Epoch   9 Batch  105/305   train_loss = 4.201\n",
      "Epoch   9 Batch  115/305   train_loss = 4.292\n",
      "Epoch   9 Batch  125/305   train_loss = 4.256\n",
      "Epoch   9 Batch  135/305   train_loss = 4.215\n",
      "Epoch   9 Batch  145/305   train_loss = 4.241\n",
      "Epoch   9 Batch  155/305   train_loss = 4.322\n",
      "Epoch   9 Batch  165/305   train_loss = 4.312\n",
      "Epoch   9 Batch  175/305   train_loss = 4.285\n",
      "Epoch   9 Batch  185/305   train_loss = 4.177\n",
      "Epoch   9 Batch  195/305   train_loss = 4.146\n",
      "Epoch   9 Batch  205/305   train_loss = 4.140\n",
      "Epoch   9 Batch  215/305   train_loss = 4.082\n",
      "Epoch   9 Batch  225/305   train_loss = 4.042\n",
      "Epoch   9 Batch  235/305   train_loss = 4.077\n",
      "Epoch   9 Batch  245/305   train_loss = 4.208\n",
      "Epoch   9 Batch  255/305   train_loss = 4.284\n",
      "Epoch   9 Batch  265/305   train_loss = 4.014\n",
      "Epoch   9 Batch  275/305   train_loss = 4.010\n",
      "Epoch   9 Batch  285/305   train_loss = 4.034\n",
      "Epoch   9 Batch  295/305   train_loss = 4.036\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(batches),\n",
    "                    train_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.save_params((seq_length, save_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
    "seq_length, load_dir = helper.load_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('بالخطوة', 0),\n",
       " ('مناتي', 1),\n",
       " ('سالكٌ', 2),\n",
       " ('تدمعي', 3),\n",
       " ('ماترخص', 4),\n",
       " ('بوحها', 5),\n",
       " ('مزقت', 6),\n",
       " ('عطها', 7),\n",
       " ('بضويك', 8),\n",
       " ('لاوربي', 9)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab_to_int.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "    \"\"\"\n",
    "    Get input, initial state, final state, and probabilities tensor from <loaded_graph>\n",
    "    :param loaded_graph: TensorFlow graph loaded from file\n",
    "    :return: Tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)\n",
    "    \"\"\"\n",
    "    InputTensor = loaded_graph.get_tensor_by_name(name='input:0')\n",
    "    InitialStateTensor = loaded_graph.get_tensor_by_name(name='initial_state:0')\n",
    "    FinalStateTensor = loaded_graph.get_tensor_by_name(name='final_state:0')\n",
    "    ProbsTensor = loaded_graph.get_tensor_by_name(name='probs:0')\n",
    "    return (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_word(preds, int_to_vocab, top_n=10):\n",
    "    \"\"\"\n",
    "    Pick the next word in the generated text\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(list(int_to_vocab.values()), 1, p=p)[0]\n",
    "    \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "عمري اسوم\n",
      " و اعديني لا يا حياتي\n",
      " انا اعرفك و اله من غيرك حياتيوايه حياتي غير رجعه\n",
      " يا ما عذرتك و ما كان يتركني عني\n",
      " من غالط احساسو صد\n",
      " انت انا قلبي حزين\n",
      " يا لي ما بتفهمني من بعد هالعمر و نضحي بناس\n",
      " انا يا سميري بحضن العبير\n",
      " و اله ما استبقيت شيئا علي الميلين عجيبة\n",
      " يا نيل يا ساحر الغيوم\n",
      " يا مالكا قلبي يا قلبي يا ليل\n",
      " يا عين يا عين الرمان يا و لاد الجيران\n",
      " و جدنا غريبين يوما\n",
      " يا له نغني يا را الغفي\n",
      " من تعافيت لا سجدنا لغزاة\n",
      " و قودا غيث همالي\n",
      " انا نرخص حمانا ذاب يا اسمر\n",
      " و يا ليت نظرها\n",
      " عاهدتني انا اكسرني من دونه\n",
      " فيذوب و ينسكب كالدمعمن المقل\n",
      " قيس اتجدين و التفاح\n",
      " كل غمره فيها حنيه اعمي اجل و سط الخزينه\n",
      " بيحرقني اني لعهد و البيعه عطتك كفوفها\n",
      " يا جزيرة يا ما يا سمين\n",
      " يا اسمر يا ليت يا قلبي من طواريها\n",
      " و انا الحن المثير البيت فتحت البيت\n",
      " بضوء العيون و رود الخد مشوار الجفاء و لا رحمتيني\n",
      " لو ما قلت و لا له\n"
     ]
    }
   ],
   "source": [
    "gen_length = 200\n",
    "prime_word = u'عمري'\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "\n",
    "    # Sentences generation setup\n",
    "    gen_sentences = [prime_word]\n",
    "\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "        \n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "        \n",
    "        pred_word = pick_word(probabilities[0][dyn_seq_length-1], int_to_vocab)\n",
    "        if pred_word == gen_sentences[len(gen_sentences)-1]:\n",
    "            continue\n",
    "        gen_sentences.append(pred_word)\n",
    "    \n",
    "    # Remove tokens\n",
    "    lyrics = ' '.join(gen_sentences)\n",
    "    for key, token in token_dict.items():\n",
    "        lyrics = lyrics.replace(' ' + token, key)\n",
    "    lyrics = re.sub('(\\n){2,}', '\\n', lyrics)\n",
    "        \n",
    "    print(lyrics.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
