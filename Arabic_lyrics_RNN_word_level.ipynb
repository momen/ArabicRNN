{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import problem_unittests as tests\n",
    "\n",
    "data_dir = 'aggregate_lyrics.txt'\n",
    "text = helper.load_data(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 109977\n",
      "The sentences 0 to 10:\n",
      "اروح لاحبابي و الاقي الفرح ساكن عينهم \n",
      " ابتسم لافراحهم و انا من الهم احترق \n",
      " و اسال جروحي من تري حس بعذابي منهم \n",
      " و بالحقيقه انصدم محدن معه همي فرق \n",
      "دورت في كل الوجيه حسيت غربه بينهم \n",
      " مع الاسف محدن ابد حس بعذاباتي و رق \n",
      " جيت اتعثر بالتعب ابي اشوف يدينهم \n",
      " ماكنت ابي الا احد يحس بي لو مانطق \n",
      " و حز فيني اني رجعت لكن رجعت بدونهم \n",
      " يحز في نفسي بانه ماسوي جرحي صدق \n"
     ]
    }
   ],
   "source": [
    "view_sentence_range = (0, 10)\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "print('The sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of lyrics split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    \n",
    "    word_counts = set(text)\n",
    "    int_to_vocab = {ii: word for ii, word in enumerate(word_counts)}\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}    \n",
    "    \n",
    "    return (vocab_to_int, int_to_vocab)\n",
    "tests.test_create_lookup_tables(create_lookup_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenize dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    tokens = {'.':'||Period||',\n",
    "              ',':'||Comma||',\n",
    "              '\"':'||Quotation_Mark||',\n",
    "              ';':'||Semicolon||',\n",
    "              '!':'||Exclamation_Mark||',\n",
    "              '؟':'||Question_Mark||',\n",
    "              '(':'||Left_Parantheses||',\n",
    "              ')':'||Right_Parantheses||',\n",
    "              '--':'||Dash||',\n",
    "              '\\n':'||Return||'\n",
    "             }\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint. If you ever decide to come back to this notebook or have to restart the notebook, you can start from here. The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "TensorFlow Version: 1.10.0\n",
      "Default GPU Device: /device:GPU:0\n"
=======
      "TensorFlow Version: 1.10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deep/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: No GPU found. Please use a GPU to train your neural network.\n",
      "  import sys\n"
>>>>>>> 0bafa60ac3d7534c22efe6fdabd4d3383aca79a2
     ]
    }
   ],
   "source": [
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, and learning rate.\n",
    "    :return: Tuple (input, targets, learning rate)\n",
    "    \"\"\"\n",
    "    inputs         = tf.placeholder(tf.int32,[None, None], name='input')\n",
    "    targets        = tf.placeholder(tf.int32, [None, None], name='target')\n",
    "    learning_rate  = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    return (inputs, targets, learning_rate)\n",
    "tests.test_get_inputs(get_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_init_cell(batch_size, rnn_size):\n",
    "    \"\"\"\n",
    "    Create an RNN Cell and initialize it.\n",
    "    :param batch_size: Size of batches\n",
    "    :param rnn_size: Size of RNNs\n",
    "    :return: Tuple (cell, initialize state)\n",
    "    \"\"\"\n",
    "   \n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    lstm_layers = 1\n",
    "    Cell = tf.contrib.rnn.MultiRNNCell([lstm] * lstm_layers)\n",
    "    InitialState = Cell.zero_state(batch_size, tf.float32)\n",
    "    InitialState = tf.identity(InitialState, name='initial_state')\n",
    "    \n",
    "    return (Cell, InitialState)\n",
    "tests.test_get_init_cell(get_init_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Create embedding for <input_data>.\n",
    "    :param input_data: TF placeholder for text input.\n",
    "    :param vocab_size: Number of words in vocabulary.\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Embedded input.\n",
    "    \"\"\"\n",
    "    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1), dtype=tf.float32)\n",
    "    embed = tf.nn.embedding_lookup(embedding, input_data)\n",
    "    \n",
    "    return embed\n",
    "tests.test_get_embed(get_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    \"\"\"\n",
    "    Create a RNN using a RNN Cell\n",
    "    :param cell: RNN Cell\n",
    "    :param inputs: Input text data\n",
    "    :return: Tuple (Outputs, Final State)\n",
    "    \"\"\"\n",
    "\n",
    "    Outputs, FinalState = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32) \n",
    "    FinalState =  tf.identity(FinalState, name='final_state')\n",
    "    return (Outputs, FinalState)\n",
    "tests.test_build_rnn(build_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size,embed_dim):\n",
    "    \"\"\"\n",
    "    Build part of the neural network\n",
    "    :param cell: RNN cell\n",
    "    :param rnn_size: Size of rnns\n",
    "    :param input_data: Input data\n",
    "    :param vocab_size: Vocabulary size\n",
    "    :return: Tuple (Logits, FinalState)\n",
    "    \"\"\"\n",
    "    \n",
    "    embedded = get_embed(input_data=input_data, vocab_size=vocab_size,embed_dim=200)\n",
    "    outputs, FinalState = build_rnn(cell=cell, inputs=embedded)\n",
    "    batch_size, embed_size = input_data.get_shape()\n",
    "    logits = tf.contrib.layers.fully_connected(outputs, vocab_size, activation_fn=None)\n",
    "\n",
    "    return (logits, FinalState)\n",
    "tests.test_build_nn(build_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target\n",
    "    :param int_text: Text with the words replaced by their ids\n",
    "    :param batch_size: The size of batch\n",
    "    :param seq_length: The length of sequence\n",
    "    :return: Batches as a Numpy array\n",
    "    \"\"\"\n",
    "    n_batches = len(int_text)//(batch_size*seq_length)\n",
    "\n",
    "    valid_text = int_text[:n_batches*batch_size*seq_length+1]\n",
    "    \n",
    "    result = np.ndarray((n_batches,2,batch_size,seq_length), dtype=int)\n",
    "    step = n_batches*seq_length    \n",
    "    \n",
    "    #print(valid_text)\n",
    "    \n",
    "    for batch in range(n_batches):\n",
    "        batch_walk = batch*seq_length\n",
    "        x = []\n",
    "        y = []\n",
    "        for binn in range(batch_size):\n",
    "            idx = batch_walk + binn * step    # start from this index\n",
    "            result[batch][0][binn] = valid_text[idx   : idx    +seq_length]\n",
    "            result[batch][1][binn] = valid_text[idx+1 : idx+1  +seq_length]   \n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_epochs = 10\n",
    "#batch_size = 512\n",
    "#seq_length = 50\n",
    "#learning_rate = 0.02\n",
    "#show_every_n_batches = 10\n",
    "#save_dir = 'save'\n",
    "\n",
    "num_epochs = 256\n",
    "# Batch Size\n",
    "batch_size = 100\n",
    "# RNN Size\n",
    "rnn_size = 512\n",
    "# Sequence Length\n",
    "seq_length = 200\n",
    "# Learning Rate\n",
    "learning_rate = 0.1\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 10\n",
    "embed_dim = 200\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 1024 and 712 for 'rnn/while/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/MatMul_1' (op: 'MatMul') with input shapes: [?,1024], [712,2048].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1575\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1576\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1577\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimensions must be equal, but are 1024 and 712 for 'rnn/while/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/MatMul_1' (op: 'MatMul') with input shapes: [?,1024], [712,2048].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-e971b2f026f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0minput_data_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_init_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrnn_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Probabilities for generating words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-b251db6ccf76>\u001b[0m in \u001b[0;36mbuild_nn\u001b[0;34m(cell, rnn_size, input_data, vocab_size)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFinalState\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfully_connected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-2335c99175f6>\u001b[0m in \u001b[0;36mbuild_rnn\u001b[0;34m(cell, inputs)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \"\"\"\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mOutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFinalState\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mFinalState\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFinalState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'final_state'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFinalState\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36mdynamic_rnn\u001b[0;34m(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0mswap_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m         dtype=dtype)\n\u001b[0m\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m     \u001b[0;31m# Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36m_dynamic_rnn_loop\u001b[0;34m(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\u001b[0m\n\u001b[1;32m    826\u001b[0m       \u001b[0mparallel_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m       \u001b[0mmaximum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       swap_memory=swap_memory)\n\u001b[0m\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m   \u001b[0;31m# Unpack final output if not using output tuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m   3230\u001b[0m       \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWHILE_CONTEXT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3231\u001b[0m     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants,\n\u001b[0;32m-> 3232\u001b[0;31m                                     return_same_structure)\n\u001b[0m\u001b[1;32m   3233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmaximum_iterations\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3234\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mBuildLoop\u001b[0;34m(self, pred, body, loop_vars, shape_invariants, return_same_structure)\u001b[0m\n\u001b[1;32m   2950\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2951\u001b[0m         original_body_result, exit_vars = self._BuildLoop(\n\u001b[0;32m-> 2952\u001b[0;31m             pred, body, original_loop_vars, loop_vars, shape_invariants)\n\u001b[0m\u001b[1;32m   2953\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2954\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36m_BuildLoop\u001b[0;34m(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\u001b[0m\n\u001b[1;32m   2885\u001b[0m         flat_sequence=vars_for_body_with_tensor_arrays)\n\u001b[1;32m   2886\u001b[0m     \u001b[0mpre_summaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2887\u001b[0;31m     \u001b[0mbody_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpacked_vars_for_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2888\u001b[0m     \u001b[0mpost_summaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2889\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i, lv)\u001b[0m\n\u001b[1;32m   3199\u001b[0m         cond = lambda i, lv: (  # pylint: disable=g-long-lambda\n\u001b[1;32m   3200\u001b[0m             math_ops.logical_and(i < maximum_iterations, orig_cond(*lv)))\n\u001b[0;32m-> 3201\u001b[0;31m         \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36m_time_step\u001b[0;34m(time, output_ta_t, state)\u001b[0m\n\u001b[1;32m    797\u001b[0m           skip_conditionals=True)\n\u001b[1;32m    798\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m       \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m     \u001b[0;31m# Pack state if using state tuples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m     \u001b[0minput_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_sequence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 785\u001b[0;31m     \u001b[0mcall_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, state, scope)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope_attrname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNNCell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_rnn_get_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m       \u001b[0;31m# Actually call layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0min_deferred_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m           raise ValueError('A layer\\'s `call` method should return a Tensor '\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, state)\u001b[0m\n\u001b[1;32m   1313\u001b[0m                                       [-1, cell.state_size])\n\u001b[1;32m   1314\u001b[0m           \u001b[0mcur_state_pos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1315\u001b[0;31m         \u001b[0mcur_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1316\u001b[0m         \u001b[0mnew_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, state, scope, *args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;31m# method.  See the class docstring for more details.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     return base_layer.Layer.__call__(self, inputs, state, scope=scope,\n\u001b[0;32m--> 329\u001b[0;31m                                      *args, **kwargs)\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m       \u001b[0;31m# Actually call layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0min_deferred_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m           raise ValueError('A layer\\'s `call` method should return a Tensor '\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, state)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m     gate_inputs = math_ops.matmul(\n\u001b[0;32m--> 628\u001b[0;31m         array_ops.concat([inputs, h], 1), self._kernel)\n\u001b[0m\u001b[1;32m    629\u001b[0m     \u001b[0mgate_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgate_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   2016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2017\u001b[0m       return gen_math_ops.mat_mul(\n\u001b[0;32m-> 2018\u001b[0;31m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   2019\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   4454\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m   4455\u001b[0m         \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4456\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   4457\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4458\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 instructions)\n\u001b[0;32m--> 454\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[1;32m    456\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3153\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3154\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3155\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3156\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3157\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1729\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1730\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1731\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[0;31m# Initialize self._outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1577\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1578\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 1024 and 712 for 'rnn/while/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/MatMul_1' (op: 'MatMul') with input shapes: [?,1024], [712,2048]."
     ]
    }
   ],
>>>>>>> 0bafa60ac3d7534c22efe6fdabd4d3383aca79a2
   "source": [
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, lr = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size)\n",
<<<<<<< HEAD
    "    logits, final_state = build_nn(cell,rnn_size, input_text, vocab_size,embed_dim)\n",
=======
    "    logits, final_state = build_nn(cell,rnn_size, input_text, vocab_size)\n",
>>>>>>> 0bafa60ac3d7534c22efe6fdabd4d3383aca79a2
    "\n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Train the neural network on the preprocessed data.  If you have a hard time getting a good loss, check the [forms](https://discussions.udacity.com/) to see if anyone is having the same problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/305   train_loss = 11.607\n",
      "Epoch   0 Batch   10/305   train_loss = 8.802\n",
      "Epoch   0 Batch   20/305   train_loss = 8.664\n",
      "Epoch   0 Batch   30/305   train_loss = 8.546\n",
      "Epoch   0 Batch   40/305   train_loss = 8.314\n",
      "Epoch   0 Batch   50/305   train_loss = 8.416\n",
      "Epoch   0 Batch   60/305   train_loss = 8.222\n",
      "Epoch   0 Batch   70/305   train_loss = 7.978\n",
      "Epoch   0 Batch   80/305   train_loss = 7.710\n",
      "Epoch   0 Batch   90/305   train_loss = 7.834\n",
      "Epoch   0 Batch  100/305   train_loss = 7.779\n",
      "Epoch   0 Batch  110/305   train_loss = 7.712\n",
      "Epoch   0 Batch  120/305   train_loss = 7.548\n",
      "Epoch   0 Batch  130/305   train_loss = 7.616\n",
      "Epoch   0 Batch  140/305   train_loss = 7.402\n",
      "Epoch   0 Batch  150/305   train_loss = 7.413\n",
      "Epoch   0 Batch  160/305   train_loss = 7.186\n",
      "Epoch   0 Batch  170/305   train_loss = 7.276\n",
      "Epoch   0 Batch  180/305   train_loss = 7.095\n",
      "Epoch   0 Batch  190/305   train_loss = 6.969\n",
      "Epoch   0 Batch  200/305   train_loss = 7.202\n",
      "Epoch   0 Batch  210/305   train_loss = 7.110\n",
      "Epoch   0 Batch  220/305   train_loss = 7.054\n",
      "Epoch   0 Batch  230/305   train_loss = 7.049\n",
      "Epoch   0 Batch  240/305   train_loss = 7.084\n",
      "Epoch   0 Batch  250/305   train_loss = 6.889\n",
      "Epoch   0 Batch  260/305   train_loss = 6.933\n",
      "Epoch   0 Batch  270/305   train_loss = 6.901\n",
      "Epoch   0 Batch  280/305   train_loss = 6.946\n",
      "Epoch   0 Batch  290/305   train_loss = 6.919\n",
      "Epoch   0 Batch  300/305   train_loss = 6.877\n",
      "Epoch   1 Batch    5/305   train_loss = 6.843\n",
      "Epoch   1 Batch   15/305   train_loss = 6.914\n",
      "Epoch   1 Batch   25/305   train_loss = 6.893\n",
      "Epoch   1 Batch   35/305   train_loss = 6.948\n",
      "Epoch   1 Batch   45/305   train_loss = 6.814\n",
      "Epoch   1 Batch   55/305   train_loss = 6.868\n",
      "Epoch   1 Batch   65/305   train_loss = 6.858\n",
      "Epoch   1 Batch   75/305   train_loss = 6.788\n",
      "Epoch   1 Batch   85/305   train_loss = 6.687\n",
      "Epoch   1 Batch   95/305   train_loss = 6.600\n",
      "Epoch   1 Batch  105/305   train_loss = 6.694\n",
      "Epoch   1 Batch  115/305   train_loss = 6.625\n",
      "Epoch   1 Batch  125/305   train_loss = 6.701\n",
      "Epoch   1 Batch  135/305   train_loss = 6.618\n",
      "Epoch   1 Batch  145/305   train_loss = 6.622\n",
      "Epoch   1 Batch  155/305   train_loss = 6.509\n",
      "Epoch   1 Batch  165/305   train_loss = 6.428\n",
      "Epoch   1 Batch  175/305   train_loss = 6.519\n",
      "Epoch   1 Batch  185/305   train_loss = 6.359\n",
      "Epoch   1 Batch  195/305   train_loss = 6.329\n",
      "Epoch   1 Batch  205/305   train_loss = 6.373\n",
      "Epoch   1 Batch  215/305   train_loss = 6.421\n",
      "Epoch   1 Batch  225/305   train_loss = 6.363\n",
      "Epoch   1 Batch  235/305   train_loss = 6.454\n",
      "Epoch   1 Batch  245/305   train_loss = 6.270\n",
      "Epoch   1 Batch  255/305   train_loss = 6.301\n",
      "Epoch   1 Batch  265/305   train_loss = 6.231\n",
      "Epoch   1 Batch  275/305   train_loss = 6.149\n",
      "Epoch   1 Batch  285/305   train_loss = 6.183\n",
      "Epoch   1 Batch  295/305   train_loss = 6.172\n",
      "Epoch   2 Batch    0/305   train_loss = 6.148\n",
      "Epoch   2 Batch   10/305   train_loss = 6.283\n",
      "Epoch   2 Batch   20/305   train_loss = 6.262\n",
      "Epoch   2 Batch   30/305   train_loss = 6.274\n",
      "Epoch   2 Batch   40/305   train_loss = 6.178\n",
      "Epoch   2 Batch   50/305   train_loss = 6.262\n",
      "Epoch   2 Batch   60/305   train_loss = 6.242\n",
      "Epoch   2 Batch   70/305   train_loss = 6.315\n",
      "Epoch   2 Batch   80/305   train_loss = 6.067\n",
      "Epoch   2 Batch   90/305   train_loss = 6.246\n",
      "Epoch   2 Batch  100/305   train_loss = 6.182\n",
      "Epoch   2 Batch  110/305   train_loss = 6.208\n",
      "Epoch   2 Batch  120/305   train_loss = 6.109\n",
      "Epoch   2 Batch  130/305   train_loss = 6.153\n",
      "Epoch   2 Batch  140/305   train_loss = 6.161\n",
      "Epoch   2 Batch  150/305   train_loss = 6.198\n",
      "Epoch   2 Batch  160/305   train_loss = 6.080\n",
      "Epoch   2 Batch  170/305   train_loss = 6.165\n",
      "Epoch   2 Batch  180/305   train_loss = 6.029\n",
      "Epoch   2 Batch  190/305   train_loss = 5.882\n",
      "Epoch   2 Batch  200/305   train_loss = 5.987\n",
      "Epoch   2 Batch  210/305   train_loss = 5.922\n",
      "Epoch   2 Batch  220/305   train_loss = 5.823\n",
      "Epoch   2 Batch  230/305   train_loss = 5.859\n",
      "Epoch   2 Batch  240/305   train_loss = 5.898\n",
      "Epoch   2 Batch  250/305   train_loss = 5.773\n",
      "Epoch   2 Batch  260/305   train_loss = 5.841\n",
      "Epoch   2 Batch  270/305   train_loss = 5.631\n",
      "Epoch   2 Batch  280/305   train_loss = 5.672\n",
      "Epoch   2 Batch  290/305   train_loss = 5.634\n",
      "Epoch   2 Batch  300/305   train_loss = 5.496\n",
      "Epoch   3 Batch    5/305   train_loss = 5.745\n",
      "Epoch   3 Batch   15/305   train_loss = 5.803\n",
      "Epoch   3 Batch   25/305   train_loss = 5.776\n",
      "Epoch   3 Batch   35/305   train_loss = 5.800\n",
      "Epoch   3 Batch   45/305   train_loss = 5.660\n",
      "Epoch   3 Batch   55/305   train_loss = 5.719\n",
      "Epoch   3 Batch   65/305   train_loss = 5.836\n",
      "Epoch   3 Batch   75/305   train_loss = 5.748\n",
      "Epoch   3 Batch   85/305   train_loss = 5.664\n",
      "Epoch   3 Batch   95/305   train_loss = 5.666\n",
      "Epoch   3 Batch  105/305   train_loss = 5.730\n",
      "Epoch   3 Batch  115/305   train_loss = 5.633\n",
      "Epoch   3 Batch  125/305   train_loss = 5.682\n",
      "Epoch   3 Batch  135/305   train_loss = 5.647\n",
      "Epoch   3 Batch  145/305   train_loss = 5.715\n",
      "Epoch   3 Batch  155/305   train_loss = 5.659\n",
      "Epoch   3 Batch  165/305   train_loss = 5.663\n",
      "Epoch   3 Batch  175/305   train_loss = 5.777\n",
      "Epoch   3 Batch  185/305   train_loss = 5.632\n",
      "Epoch   3 Batch  195/305   train_loss = 5.518\n",
      "Epoch   3 Batch  205/305   train_loss = 5.468\n",
      "Epoch   3 Batch  215/305   train_loss = 5.549\n",
      "Epoch   3 Batch  225/305   train_loss = 5.417\n",
      "Epoch   3 Batch  235/305   train_loss = 5.430\n",
      "Epoch   3 Batch  245/305   train_loss = 5.397\n",
      "Epoch   3 Batch  255/305   train_loss = 5.444\n",
      "Epoch   3 Batch  265/305   train_loss = 5.286\n",
      "Epoch   3 Batch  275/305   train_loss = 5.154\n",
      "Epoch   3 Batch  285/305   train_loss = 5.198\n",
      "Epoch   3 Batch  295/305   train_loss = 5.205\n",
      "Epoch   4 Batch    0/305   train_loss = 5.255\n",
      "Epoch   4 Batch   10/305   train_loss = 5.337\n",
      "Epoch   4 Batch   20/305   train_loss = 5.351\n",
      "Epoch   4 Batch   30/305   train_loss = 5.336\n",
      "Epoch   4 Batch   40/305   train_loss = 5.279\n",
      "Epoch   4 Batch   50/305   train_loss = 5.300\n",
      "Epoch   4 Batch   60/305   train_loss = 5.328\n",
      "Epoch   4 Batch   70/305   train_loss = 5.390\n",
      "Epoch   4 Batch   80/305   train_loss = 5.335\n",
      "Epoch   4 Batch   90/305   train_loss = 5.387\n",
      "Epoch   4 Batch  100/305   train_loss = 5.373\n",
      "Epoch   4 Batch  110/305   train_loss = 5.294\n",
      "Epoch   4 Batch  120/305   train_loss = 5.269\n",
      "Epoch   4 Batch  130/305   train_loss = 5.252\n",
      "Epoch   4 Batch  140/305   train_loss = 5.313\n",
      "Epoch   4 Batch  150/305   train_loss = 5.360\n",
      "Epoch   4 Batch  160/305   train_loss = 5.305\n",
      "Epoch   4 Batch  170/305   train_loss = 5.453\n",
      "Epoch   4 Batch  180/305   train_loss = 5.309\n",
      "Epoch   4 Batch  190/305   train_loss = 5.108\n",
      "Epoch   4 Batch  200/305   train_loss = 5.138\n",
      "Epoch   4 Batch  210/305   train_loss = 5.085\n",
      "Epoch   4 Batch  220/305   train_loss = 5.045\n",
      "Epoch   4 Batch  230/305   train_loss = 5.042\n",
      "Epoch   4 Batch  240/305   train_loss = 5.092\n",
      "Epoch   4 Batch  250/305   train_loss = 5.044\n",
      "Epoch   4 Batch  260/305   train_loss = 5.122\n",
      "Epoch   4 Batch  270/305   train_loss = 4.924\n",
      "Epoch   4 Batch  280/305   train_loss = 4.929\n",
      "Epoch   4 Batch  290/305   train_loss = 4.866\n",
      "Epoch   4 Batch  300/305   train_loss = 4.854\n",
      "Epoch   5 Batch    5/305   train_loss = 4.932\n",
      "Epoch   5 Batch   15/305   train_loss = 5.017\n",
      "Epoch   5 Batch   25/305   train_loss = 5.069\n",
      "Epoch   5 Batch   35/305   train_loss = 4.996\n",
      "Epoch   5 Batch   45/305   train_loss = 4.885\n",
      "Epoch   5 Batch   55/305   train_loss = 4.908\n",
      "Epoch   5 Batch   65/305   train_loss = 5.094\n",
      "Epoch   5 Batch   75/305   train_loss = 5.031\n",
      "Epoch   5 Batch   85/305   train_loss = 5.058\n",
      "Epoch   5 Batch   95/305   train_loss = 4.953\n",
      "Epoch   5 Batch  105/305   train_loss = 5.033\n",
      "Epoch   5 Batch  115/305   train_loss = 4.955\n",
      "Epoch   5 Batch  125/305   train_loss = 5.016\n",
      "Epoch   5 Batch  135/305   train_loss = 4.991\n",
      "Epoch   5 Batch  145/305   train_loss = 4.995\n",
      "Epoch   5 Batch  155/305   train_loss = 5.029\n",
      "Epoch   5 Batch  165/305   train_loss = 5.079\n",
      "Epoch   5 Batch  175/305   train_loss = 5.123\n",
      "Epoch   5 Batch  185/305   train_loss = 4.995\n",
      "Epoch   5 Batch  195/305   train_loss = 4.871\n",
      "Epoch   5 Batch  205/305   train_loss = 4.858\n",
      "Epoch   5 Batch  215/305   train_loss = 4.856\n",
      "Epoch   5 Batch  225/305   train_loss = 4.810\n",
      "Epoch   5 Batch  235/305   train_loss = 4.825\n",
      "Epoch   5 Batch  245/305   train_loss = 4.845\n",
      "Epoch   5 Batch  255/305   train_loss = 4.916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5 Batch  265/305   train_loss = 4.740\n",
      "Epoch   5 Batch  275/305   train_loss = 4.622\n",
      "Epoch   5 Batch  285/305   train_loss = 4.646\n",
      "Epoch   5 Batch  295/305   train_loss = 4.701\n",
      "Epoch   6 Batch    0/305   train_loss = 4.748\n",
      "Epoch   6 Batch   10/305   train_loss = 4.738\n",
      "Epoch   6 Batch   20/305   train_loss = 4.783\n",
      "Epoch   6 Batch   30/305   train_loss = 4.783\n",
      "Epoch   6 Batch   40/305   train_loss = 4.764\n",
      "Epoch   6 Batch   50/305   train_loss = 4.682\n",
      "Epoch   6 Batch   60/305   train_loss = 4.755\n",
      "Epoch   6 Batch   70/305   train_loss = 4.827\n",
      "Epoch   6 Batch   80/305   train_loss = 4.863\n",
      "Epoch   6 Batch   90/305   train_loss = 4.833\n",
      "Epoch   6 Batch  100/305   train_loss = 4.852\n",
      "Epoch   6 Batch  110/305   train_loss = 4.771\n",
      "Epoch   6 Batch  120/305   train_loss = 4.760\n",
      "Epoch   6 Batch  130/305   train_loss = 4.712\n",
      "Epoch   6 Batch  140/305   train_loss = 4.828\n",
      "Epoch   6 Batch  150/305   train_loss = 4.770\n",
      "Epoch   6 Batch  160/305   train_loss = 4.718\n",
      "Epoch   6 Batch  170/305   train_loss = 4.940\n",
      "Epoch   6 Batch  180/305   train_loss = 4.854\n",
      "Epoch   6 Batch  190/305   train_loss = 4.633\n",
      "Epoch   6 Batch  200/305   train_loss = 4.596\n",
      "Epoch   6 Batch  210/305   train_loss = 4.572\n",
      "Epoch   6 Batch  220/305   train_loss = 4.550\n",
      "Epoch   6 Batch  230/305   train_loss = 4.594\n",
      "Epoch   6 Batch  240/305   train_loss = 4.621\n",
      "Epoch   6 Batch  250/305   train_loss = 4.627\n",
      "Epoch   6 Batch  260/305   train_loss = 4.663\n",
      "Epoch   6 Batch  270/305   train_loss = 4.536\n",
      "Epoch   6 Batch  280/305   train_loss = 4.529\n",
      "Epoch   6 Batch  290/305   train_loss = 4.421\n",
      "Epoch   6 Batch  300/305   train_loss = 4.443\n",
      "Epoch   7 Batch    5/305   train_loss = 4.564\n",
      "Epoch   7 Batch   15/305   train_loss = 4.554\n",
      "Epoch   7 Batch   25/305   train_loss = 4.612\n",
      "Epoch   7 Batch   35/305   train_loss = 4.587\n",
      "Epoch   7 Batch   45/305   train_loss = 4.483\n",
      "Epoch   7 Batch   55/305   train_loss = 4.462\n",
      "Epoch   7 Batch   65/305   train_loss = 4.598\n",
      "Epoch   7 Batch   75/305   train_loss = 4.582\n",
      "Epoch   7 Batch   85/305   train_loss = 4.616\n",
      "Epoch   7 Batch   95/305   train_loss = 4.470\n",
      "Epoch   7 Batch  105/305   train_loss = 4.608\n",
      "Epoch   7 Batch  115/305   train_loss = 4.559\n",
      "Epoch   7 Batch  125/305   train_loss = 4.551\n",
      "Epoch   7 Batch  135/305   train_loss = 4.549\n",
      "Epoch   7 Batch  145/305   train_loss = 4.568\n",
      "Epoch   7 Batch  155/305   train_loss = 4.624\n",
      "Epoch   7 Batch  165/305   train_loss = 4.658\n",
      "Epoch   7 Batch  175/305   train_loss = 4.634\n",
      "Epoch   7 Batch  185/305   train_loss = 4.566\n",
      "Epoch   7 Batch  195/305   train_loss = 4.452\n",
      "Epoch   7 Batch  205/305   train_loss = 4.438\n",
      "Epoch   7 Batch  215/305   train_loss = 4.423\n",
      "Epoch   7 Batch  225/305   train_loss = 4.379\n",
      "Epoch   7 Batch  235/305   train_loss = 4.432\n",
      "Epoch   7 Batch  245/305   train_loss = 4.465\n",
      "Epoch   7 Batch  255/305   train_loss = 4.577\n",
      "Epoch   7 Batch  265/305   train_loss = 4.334\n",
      "Epoch   7 Batch  275/305   train_loss = 4.288\n",
      "Epoch   7 Batch  285/305   train_loss = 4.282\n",
      "Epoch   7 Batch  295/305   train_loss = 4.316\n",
      "Epoch   8 Batch    0/305   train_loss = 4.380\n",
      "Epoch   8 Batch   10/305   train_loss = 4.379\n",
      "Epoch   8 Batch   20/305   train_loss = 4.384\n",
      "Epoch   8 Batch   30/305   train_loss = 4.333\n",
      "Epoch   8 Batch   40/305   train_loss = 4.439\n",
      "Epoch   8 Batch   50/305   train_loss = 4.325\n",
      "Epoch   8 Batch   60/305   train_loss = 4.367\n",
      "Epoch   8 Batch   70/305   train_loss = 4.446\n",
      "Epoch   8 Batch   80/305   train_loss = 4.498\n",
      "Epoch   8 Batch   90/305   train_loss = 4.442\n",
      "Epoch   8 Batch  100/305   train_loss = 4.424\n",
      "Epoch   8 Batch  110/305   train_loss = 4.388\n",
      "Epoch   8 Batch  120/305   train_loss = 4.412\n",
      "Epoch   8 Batch  130/305   train_loss = 4.349\n",
      "Epoch   8 Batch  140/305   train_loss = 4.471\n",
      "Epoch   8 Batch  150/305   train_loss = 4.422\n",
      "Epoch   8 Batch  160/305   train_loss = 4.342\n",
      "Epoch   8 Batch  170/305   train_loss = 4.531\n",
      "Epoch   8 Batch  180/305   train_loss = 4.438\n",
      "Epoch   8 Batch  190/305   train_loss = 4.305\n",
      "Epoch   8 Batch  200/305   train_loss = 4.263\n",
      "Epoch   8 Batch  210/305   train_loss = 4.239\n",
      "Epoch   8 Batch  220/305   train_loss = 4.183\n",
      "Epoch   8 Batch  230/305   train_loss = 4.242\n",
      "Epoch   8 Batch  240/305   train_loss = 4.269\n",
      "Epoch   8 Batch  250/305   train_loss = 4.305\n",
      "Epoch   8 Batch  260/305   train_loss = 4.381\n",
      "Epoch   8 Batch  270/305   train_loss = 4.261\n",
      "Epoch   8 Batch  280/305   train_loss = 4.205\n",
      "Epoch   8 Batch  290/305   train_loss = 4.139\n",
      "Epoch   8 Batch  300/305   train_loss = 4.123\n",
      "Epoch   9 Batch    5/305   train_loss = 4.211\n",
      "Epoch   9 Batch   15/305   train_loss = 4.240\n",
      "Epoch   9 Batch   25/305   train_loss = 4.308\n",
      "Epoch   9 Batch   35/305   train_loss = 4.209\n",
      "Epoch   9 Batch   45/305   train_loss = 4.170\n",
      "Epoch   9 Batch   55/305   train_loss = 4.177\n",
      "Epoch   9 Batch   65/305   train_loss = 4.259\n",
      "Epoch   9 Batch   75/305   train_loss = 4.242\n",
      "Epoch   9 Batch   85/305   train_loss = 4.283\n",
      "Epoch   9 Batch   95/305   train_loss = 4.194\n",
      "Epoch   9 Batch  105/305   train_loss = 4.201\n",
      "Epoch   9 Batch  115/305   train_loss = 4.292\n",
      "Epoch   9 Batch  125/305   train_loss = 4.256\n",
      "Epoch   9 Batch  135/305   train_loss = 4.215\n",
      "Epoch   9 Batch  145/305   train_loss = 4.241\n",
      "Epoch   9 Batch  155/305   train_loss = 4.322\n",
      "Epoch   9 Batch  165/305   train_loss = 4.312\n",
      "Epoch   9 Batch  175/305   train_loss = 4.285\n",
      "Epoch   9 Batch  185/305   train_loss = 4.177\n",
      "Epoch   9 Batch  195/305   train_loss = 4.146\n",
      "Epoch   9 Batch  205/305   train_loss = 4.140\n",
      "Epoch   9 Batch  215/305   train_loss = 4.082\n",
      "Epoch   9 Batch  225/305   train_loss = 4.042\n",
      "Epoch   9 Batch  235/305   train_loss = 4.077\n",
      "Epoch   9 Batch  245/305   train_loss = 4.208\n",
      "Epoch   9 Batch  255/305   train_loss = 4.284\n",
      "Epoch   9 Batch  265/305   train_loss = 4.014\n",
      "Epoch   9 Batch  275/305   train_loss = 4.010\n",
      "Epoch   9 Batch  285/305   train_loss = 4.034\n",
      "Epoch   9 Batch  295/305   train_loss = 4.036\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(batches),\n",
    "                    train_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.save_params((seq_length, save_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
    "seq_length, load_dir = helper.load_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('بالخطوة', 0),\n",
       " ('مناتي', 1),\n",
       " ('سالكٌ', 2),\n",
       " ('تدمعي', 3),\n",
       " ('ماترخص', 4),\n",
       " ('بوحها', 5),\n",
       " ('مزقت', 6),\n",
       " ('عطها', 7),\n",
       " ('بضويك', 8),\n",
       " ('لاوربي', 9)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab_to_int.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "    \"\"\"\n",
    "    Get input, initial state, final state, and probabilities tensor from <loaded_graph>\n",
    "    :param loaded_graph: TensorFlow graph loaded from file\n",
    "    :return: Tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)\n",
    "    \"\"\"\n",
    "    InputTensor = loaded_graph.get_tensor_by_name(name='input:0')\n",
    "    InitialStateTensor = loaded_graph.get_tensor_by_name(name='initial_state:0')\n",
    "    FinalStateTensor = loaded_graph.get_tensor_by_name(name='final_state:0')\n",
    "    ProbsTensor = loaded_graph.get_tensor_by_name(name='probs:0')\n",
    "    return (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_word(preds, int_to_vocab, top_n=10):\n",
    "    \"\"\"\n",
    "    Pick the next word in the generated text\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(list(int_to_vocab.values()), 1, p=p)[0]\n",
    "    \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "عمري اسوم\n",
      " و اعديني لا يا حياتي\n",
      " انا اعرفك و اله من غيرك حياتيوايه حياتي غير رجعه\n",
      " يا ما عذرتك و ما كان يتركني عني\n",
      " من غالط احساسو صد\n",
      " انت انا قلبي حزين\n",
      " يا لي ما بتفهمني من بعد هالعمر و نضحي بناس\n",
      " انا يا سميري بحضن العبير\n",
      " و اله ما استبقيت شيئا علي الميلين عجيبة\n",
      " يا نيل يا ساحر الغيوم\n",
      " يا مالكا قلبي يا قلبي يا ليل\n",
      " يا عين يا عين الرمان يا و لاد الجيران\n",
      " و جدنا غريبين يوما\n",
      " يا له نغني يا را الغفي\n",
      " من تعافيت لا سجدنا لغزاة\n",
      " و قودا غيث همالي\n",
      " انا نرخص حمانا ذاب يا اسمر\n",
      " و يا ليت نظرها\n",
      " عاهدتني انا اكسرني من دونه\n",
      " فيذوب و ينسكب كالدمعمن المقل\n",
      " قيس اتجدين و التفاح\n",
      " كل غمره فيها حنيه اعمي اجل و سط الخزينه\n",
      " بيحرقني اني لعهد و البيعه عطتك كفوفها\n",
      " يا جزيرة يا ما يا سمين\n",
      " يا اسمر يا ليت يا قلبي من طواريها\n",
      " و انا الحن المثير البيت فتحت البيت\n",
      " بضوء العيون و رود الخد مشوار الجفاء و لا رحمتيني\n",
      " لو ما قلت و لا له\n"
     ]
    }
   ],
   "source": [
    "gen_length = 200\n",
    "prime_word = u'عمري'\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "\n",
    "    # Sentences generation setup\n",
    "    gen_sentences = [prime_word]\n",
    "\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "        \n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "        \n",
    "        pred_word = pick_word(probabilities[0][dyn_seq_length-1], int_to_vocab)\n",
    "        if pred_word == gen_sentences[len(gen_sentences)-1]:\n",
    "            continue\n",
    "        gen_sentences.append(pred_word)\n",
    "    \n",
    "    # Remove tokens\n",
    "    lyrics = ' '.join(gen_sentences)\n",
    "    for key, token in token_dict.items():\n",
    "        lyrics = lyrics.replace(' ' + token, key)\n",
    "    lyrics = re.sub('(\\n){2,}', '\\n', lyrics)\n",
    "        \n",
    "    print(lyrics.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
